{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS5340 Lecture 2:  MLE, MAP and Bayesian Methods#\n",
    "by Harold Soh (harold@comp.nus.edu.sg)\n",
    "\n",
    "Graduate TAs: Xie Yaqi and Abdul Fatir Ansari\n",
    "\n",
    "This notebook is a supplement to Lecture 2 of CS5340: Uncertainty Modeling in AI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "from scipy import optimize\n",
    "\n",
    "from numpy.random import randn\n",
    "from numpy import log, exp\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "import scipy.stats as stats\n",
    "import scipy.special"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create some data from our simulated ranger #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create some data (this is our simulated sensor)\n",
    "N = 3\n",
    "r = 1 # our object is 1m away\n",
    "np.random.seed(0)\n",
    "\n",
    "# true parameters\n",
    "mu_true = 0.1\n",
    "var_true = 0.05\n",
    "\n",
    "# some noise \n",
    "x = np.sqrt(var_true)*np.random.randn(N,1) + mu_true\n",
    "\n",
    "# our model\n",
    "y =  r + x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(y, 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning the Parameters #\n",
    "In this section, we'll use the theory learnt in lecture (MLE, Bayesian, and MAP) to learn the parameters of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum Likelihood Estimation (MLE) ##\n",
    "First, we'll derive the maximum likelihood estimates for $\\mu$ and $\\sigma^2$ using the closed-form equations developed in class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# MLE estimate\n",
    "# closed form\n",
    "mu_MLE = np.mean(x)\n",
    "var_MLE = np.mean( (x)**2 ) - (mu_MLE)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"mu_MLE = %g, var_MLE = %g\"%(mu_MLE, var_MLE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try using an off-the-shelf optimizer to numerically *minimize* the *negative* log-likehood directly (equivalent to *maximizing* the log likelihood). It should give us the same answer (or something really close)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# using an optimizer\n",
    "def negloglike(theta, x):\n",
    "    mu = theta[0]\n",
    "    var = exp(theta[1])\n",
    "    N = x.size\n",
    "    nll = N*log(var) + np.sum((x - mu)**2/var)\n",
    "    return nll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = optimize.minimize(negloglike, np.array([1, 2]), args=(x), method='BFGS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(params)\n",
    "\n",
    "mu_MLEopt = params.x[0]\n",
    "var_MLEopt = exp(params.x[1])\n",
    "\n",
    "print(\"--------------------------------------------------------\")\n",
    "print(\"mu_MLEopt = %g, var_MLEopt = %g\"%(mu_MLEopt, var_MLEopt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot $p(D|\\mu, \\sigma^2)$ and examine where the MLE is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 0.01\n",
    "mp = np.arange(-0.5, 0.5, d)\n",
    "sp = np.arange(0.01, 0.5, d) #standard deviation\n",
    "Mp, Sp = np.meshgrid(mp, sp)\n",
    "Zlike = np.zeros(Mp.shape)\n",
    "for i in range(Mp.shape[0]):\n",
    "    for j in range(Mp.shape[1]):\n",
    "        Zlike[i,j] = exp(np.sum(stats.norm.logpdf(x, loc=Mp[i,j], scale=Sp[i,j])))\n",
    "\n",
    "def plotDist(M, S, Z, title):\n",
    "    fig, ax = plt.subplots()\n",
    "    CS = ax.contourf(M, S**2, Z, 100)\n",
    "    #ax.clabel(CS, inline=1, fontsize=10)\n",
    "    ax.set_title(title)\n",
    "    fig.colorbar(CS, ax=ax)\n",
    "    ax.set_xlabel('$\\mu$')\n",
    "    ax.set_ylabel('$\\sigma^2$')\n",
    "    return (fig,ax)\n",
    "\n",
    "plotDist(Mp, Sp, Zlike, 'Likelihood: $p(D|\\mu, \\sigma^2)$')\n",
    "plt.scatter(mu_true, var_true, marker='+', color='w')\n",
    "plt.scatter(mu_MLE, var_MLE, marker='x', color=\"r\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Approach #\n",
    "Next, let's try full Bayesian posterior estimation. First, we define the parameters of our prior (a NormalInverseGamma[$\\delta, \\gamma, \\alpha, \\beta$]) and plot the prior to get an idea of how it looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prior parameters\n",
    "delta = 0.0 # mean\n",
    "gamma = 1.0\n",
    "\n",
    "# put mode of variance at 0.05 \n",
    "alpha = 5.0\n",
    "beta = 0.3\n",
    "\n",
    "mode = beta/(alpha + 1)\n",
    "mean = beta/(alpha - 1)\n",
    "print(\"Mode: %g, Mean: %g\"%(mode, mean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot the prior\n",
    "Zp = np.zeros(Mp.shape)\n",
    "for i in range(Mp.shape[0]):\n",
    "    for j in range(Mp.shape[1]):\n",
    "        Zp[i,j] = exp(\n",
    "            stats.norm.logpdf(Mp[i,j], loc=delta, scale=Sp[i,j]/np.sqrt(gamma)) +\n",
    "            stats.invgamma.logpdf(Sp[i,j]**2, alpha, scale=beta)\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plotDist(Mp, Sp, Zp, 'Prior: $p(\\mu, \\sigma^2|\\delta,\\gamma,a,b)$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the *unnormalized posterior* to take a look (this is the numerator $p(D|\\mu, \\sigma^2)p(\\mu, \\sigma^2|\\delta,\\gamma,a,b)$ of the RHS of the Bayes update rule, i.e., before normalizing with $p(D)$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Znum = np.zeros(Mp.shape)\n",
    "for i in range(Mp.shape[0]):\n",
    "    for j in range(Mp.shape[1]):\n",
    "        Znum[i,j] = exp(\n",
    "            np.sum(stats.norm.logpdf(x, loc=Mp[i,j], scale=Sp[i,j])) +\n",
    "            stats.norm.logpdf(Mp[i,j], loc=delta, scale=Sp[i,j]/np.sqrt(gamma)) +\n",
    "            stats.invgamma.logpdf(Sp[i,j]**2, alpha, scale=beta)\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotDist(Mp, Sp, Zlike, 'Likelihood: $p(D|\\mu, \\sigma^2)$')\n",
    "plt.scatter(mu_true, var_true, marker='+', color='w')\n",
    "plt.scatter(mu_MLE, var_MLE, marker='x', color=\"r\")\n",
    "\n",
    "\n",
    "plotDist(Mp, Sp, Znum, 'Unnormalized posterior: $p(D|\\mu, \\sigma^2)p(\\mu, \\sigma^2|\\delta,\\gamma,a,b)$')\n",
    "plt.scatter(mu_true, var_true, marker='+', color='w')\n",
    "plt.scatter(mu_MLE, var_MLE, marker='x', color=\"r\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can update the posterior in closed form as described in the lecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xbar = np.mean(x)\n",
    "\n",
    "delta_post = (delta*gamma + N*xbar)/(gamma + N) # mean\n",
    "gamma_post = gamma + N\n",
    "alpha_post = alpha + N/2\n",
    "beta_post = beta + np.sum( (x- xbar)**2)/2 + ((gamma*N)/(gamma + N))*((xbar - delta)**2/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Posteriors:\\n delta = %g\\n gamma = %g\\n alpha = %g\\n beta = %g\\n\"%(delta_post, gamma_post, alpha_post, beta_post))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot this posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Zpost = np.zeros(Mp.shape)\n",
    "for i in range(Mp.shape[0]):\n",
    "    for j in range(Mp.shape[1]):\n",
    "        Zpost[i,j] = exp(\n",
    "            stats.norm.logpdf(Mp[i,j], loc=delta_post, scale=Sp[i,j]/np.sqrt(gamma_post)) +\n",
    "            stats.invgamma.logpdf(Sp[i,j]**2, alpha_post, scale=beta_post)\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotDist(Mp, Sp, Zpost, 'Posterior')\n",
    "plt.scatter(mu_true, var_true, marker='+', color='w')\n",
    "plt.scatter(mu_MLE, var_MLE, marker='x', color=\"r\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum a Posteriori (MAP) Estimation ##\n",
    "Finally, let's try MAP estimation which is a \"middle ground\" approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# closed form\n",
    "mu_MAP = (N*np.mean(x) + gamma*delta)/(N+gamma)  \n",
    "var_MAP = (np.sum((x - mu_MAP)**2) + 2*beta + gamma*(delta - mu_MAP)**2)/(N + 3 + 2*alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"mu_MAP = %g, var_MAP = %g\"%(mu_MAP, var_MAP))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we use an optimizer which should give us very similar results to the above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# using an optimizer\n",
    "def neglogpost(theta, x):\n",
    "    mu = theta[0]\n",
    "    var = exp(theta[1])\n",
    "    N = x.size\n",
    "    nlpost = (log(var)*(N+ (2*alpha) + 3)/2) + ((np.sum((x - mu)**2) +  2*beta + (gamma*(delta - mu)**2))/(2*var))\n",
    "    return nlpost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neglogpost([mu_MAP, log(var_MAP)], x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = optimize.minimize(neglogpost, np.array([1, 2.0]), args=(x), \n",
    "                           method='BFGS'\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(params)\n",
    "\n",
    "mu_MAPopt = params.x[0]\n",
    "var_MAPopt = exp(params.x[1])\n",
    "\n",
    "print(\"mu_MAPopt = %g, var_MAPopt = %g\"%(mu_MAPopt, var_MAPopt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also plot the MAP estimate to see where it falls relative to the MLE and Bayesian posterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plotDist(Mp, Sp, Zpost, 'Posterior')\n",
    "plt.scatter(mu_MLE, var_MLE, marker='x')\n",
    "plt.scatter(mu_MAP, var_MAP, marker='.')\n",
    "plt.scatter(mu_true, var_true, marker='+', color='w')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making Predictions #\n",
    "\n",
    "Suppose there is a obstable 2.5m away from our sensor. Since our sensor is imperfect, it returns some reading $\\hat{y} = r + x$ where $x$ is drawn from $N(\\mu, \\sigma^2)$. Let us estimate the real distance of this obstacle based on our earlier computed parameters and this observation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_true = 2.0 # you can change this\n",
    "X = scipy.stats.norm(loc=mu_true, scale=np.sqrt(var_true))\n",
    "yhat = r_true + X.rvs(1)\n",
    "print(yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLE and MAP Predictions##\n",
    "Recall that $X \\sim N(\\mu, \\sigma^2)$. Let the range to the obstacle $R = \\hat{y} - X$. R is also normally-distributed, $R \\sim N(\\hat{y} - \\mu, \\sigma^2)$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# closed form\n",
    "r_MLE = yhat - mu_MLE\n",
    "r_MAP = yhat - mu_MAP\n",
    "\n",
    "print(\" r_MLE = %g \\n r_MAP = %g\"%(r_MLE, r_MAP))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = np.arange(1.0, 3, 0.001)\n",
    "fig, ax = plt.subplots(1,2, figsize=(10,4))\n",
    "ax[0].plot(z, stats.norm.pdf(z, loc=r_MLE, scale=var_MLE), label=\"MLE\")\n",
    "ax[0].plot(z, stats.norm.pdf(z, loc=r_MAP, scale=var_MAP), label=\"MAP\")\n",
    "ax[0].axvline(x=r_true, color='red', lw=1)\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].semilogy(z, stats.norm.pdf(z, loc=r_MLE, scale=var_MLE), label=\"MLE\")\n",
    "ax[1].semilogy(z, stats.norm.pdf(z, loc=r_MAP, scale=var_MAP), label=\"MAP\")\n",
    "ax[1].axvline(x=r_true, color='red', lw=1)\n",
    "ax[1].legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Predictions ##\n",
    "The posterior predictive is actually a generalized Student-t distribution. The exact form is given in the lecture slides and on wikipedia: https://en.wikipedia.org/wiki/Student%27s_t-distribution#Generalized_Student's_t-distribution \n",
    "\n",
    "The posterior parameters are at: https://en.wikipedia.org/wiki/Conjugate_prior#Continuous_distributions\n",
    "\n",
    "Given our model, $R$ is also Student-t distributed but with location parameter $\\hat{y} - \\mu_T$ where $\\mu_T$ is the location parameter of our Student-T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tpdf(x, delta, gamma, alpha, beta):\n",
    "    xbar = np.mean(x)\n",
    "    mu_T = delta\n",
    "    scale_T = beta*(gamma + 1)/(gamma*alpha)\n",
    "    dof = 2*alpha\n",
    "    \n",
    "    doft = (dof + 1)/2\n",
    "    \n",
    "    pref = scipy.special.gamma(doft)/(scipy.special.gamma(dof/2)*np.sqrt(np.pi*dof)*scale_T)\n",
    "    p = pref*(1 + (1/dof)*((x - mu_T)/scale_T)**2)**(-doft)\n",
    "    \n",
    "    return p\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "z = np.arange(1.0, 3.0, 0.001)\n",
    "fig, ax = plt.subplots(1,2, figsize=(10,4))\n",
    "ax[0].plot(z, stats.norm.pdf(z, loc=r_MLE, scale=var_MLE), label=\"MLE\")\n",
    "ax[0].plot(z, stats.norm.pdf(z, loc=r_MAP, scale=var_MAP), label=\"MAP\")\n",
    "ax[0].plot(z, tpdf(z,yhat - delta_post,gamma_post, alpha_post , beta_post), label=\"Bayes\")\n",
    "\n",
    "ax[0].axvline(x=r_true, color='red', lw=1)\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].semilogy(z, stats.norm.pdf(z, loc=r_MLE, scale=var_MLE), label=\"MLE\")\n",
    "ax[1].semilogy(z, stats.norm.pdf(z, loc=r_MAP, scale=var_MAP), label=\"MAP\")\n",
    "ax[1].semilogy(z, tpdf(z,yhat - delta_post,gamma_post, alpha_post , beta_post), label=\"Bayes\")\n",
    "\n",
    "ax[1].axvline(x=r_true, color='red', lw=1)\n",
    "ax[1].legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have set a low number of observations (e.g., $N = 3$), you'll likely see  MAP would typically be overconfident relative to the Bayes estimate (given a broad prior). If you selected a high number of observations, say $N=300$, the estimates should match to a good degree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
